# Positioning Brief

## Current State Assessment

The live portfolio presents Samruddhi as a "Lead Designer" executing conversational design projects. The language throughout signals execution over ownership: "architecting flows," "crafting microcopy," "designing for panic." The site reads as a UX research portfolio with some AI work, not as an AI systems ownership portfolio.

**Critical misalignments:**

- **Title framing**: "Lead Designer" / "Conversational Designer" language throughout
- **Work hierarchy**: ICICI voicebot (the strongest systems evidence) is item #1 in a five-project grid with equal visual weight given to academic research projects
- **Missing flagship work**: CS1 (Audit-Safe Conversations) and CS2 (Designing for Panic) — which demonstrate decision logic, compliance architecture, and escalation policy — don't appear on the live site at all
- **Project framing**: SmartCoin reads as UX research ("understanding borrowing," "sculpting personas"), not as automation behavior design. Vihar and Bhopal Bikes are service design research projects with no automation component
- **Language patterns**: "Design principles," "user journey," "behavioral statement" — all UX research terminology, not systems architecture language

**What's being undersold:** The ICICI work represents transactional AI handling 70+ financial flows with regulatory compliance requirements. The WhatsApp dispute resolution system (CS1/CS2) demonstrates behavior ownership at scale — defining when automation can act vs. when it must escalate, building explicit confirmation loops to prevent stale consent, designing UI as compliance safeguards. This is product management and systems architecture work framed as design execution.

**What's being misframed:** Dottie (IndiGo speculative project) has equal prominence to ICICI Bank real production work. Academic research projects (Van Vihar visitor discovery, Bhopal bike-sharing communication) dilute the AI systems positioning.

---

## Target Audience Profiles

### Buyer 1: Director of Automation (Fintech)

**Problem they're solving by hiring:**
Their engineering team can build the technical infrastructure for conversational AI, but nobody owns the boundary logic: when should the bot collect more information vs. escalate to human? What confidence threshold justifies auto-resolution vs. creating a service request? How do regulatory windows (TAT, cooling periods) translate into conversation states?

**What triggers trust immediately:**
- Evidence of working within compliance frameworks (NPCI, RBI, financial regulations)
- Proof of defining escalation policies that prevented operational chaos
- Understanding that "fast automation that makes mistakes is more expensive than slow automation that gets it right"
- Speaking in terms of containment rates, false positive reduction, regulatory exposure mitigation

**What triggers dismissal immediately:**
- Portfolio that reads as "I designed conversation flows" rather than "I defined decision boundaries"
- Speculative projects given equal weight to production systems
- UX research language without operational outcomes
- No evidence of collaboration with engineering, compliance, or data science teams

---

### Buyer 2: Product Lead for Enterprise AI

**Problem they're solving by hiring:**
They need someone who can translate messy real-world scenarios into structured bot behavior: How do you disambiguate between "failed UPI debit," "recurring mandate," and "credit card subscription" when users describe them identically? How do you build progressive disclosure in high-anxiety moments? How do you design confirmation loops that satisfy audit requirements without adding friction?

**What triggers trust immediately:**
- Case studies showing disambiguation logic and decision trees
- Evidence of thinking about edge cases, ambiguity handling, and graceful degradation
- Understanding of async channel constraints (WhatsApp session management, stale consent risk)
- Concrete examples of "we refused to allow X because regulatory risk outweighed user convenience"

**What triggers dismissal immediately:**
- Focus on personality and brand voice over logic and safety
- "Delightful experiences" language in financial contexts
- No discussion of fallback strategies or error states
- Designing for the happy path without addressing failure modes

---

### Buyer 3: AI Systems Lead (Non-Engineering Role)

**Problem they're solving by hiring:**
They have ML engineers and software engineers but need a "behavior specification" partner — someone who can define what the system should do at the product logic layer, then collaborate with technical teams to implement it. Not a PM who writes PRDs. Not a designer who makes flows. Someone who owns the decision logic.

**What triggers trust immediately:**
- Language like "defined confidence thresholds," "built escalation taxonomy," "specified containment criteria"
- Evidence of working with data science on model performance metrics (precision/recall tradeoffs in real scenarios)
- Proof of creating behavior frameworks that engineering teams can implement
- Understanding of annotation, intent hierarchies, entity disambiguation

**What triggers dismissal immediately:**
- Job title with "Designer" in it (signals execution, not ownership)
- Portfolio showing mockups and journey maps without system logic
- Focus on user empathy over operational constraints
- No evidence of quantitative thinking (metrics, thresholds, error rates)

---

## The Narrative Arc

Samruddhi saw the same pattern across every AI system she worked on: the gap between what a user says and what a system is legally and operationally permitted to do. In financial services, that gap is massive — a user saying "money is missing" could mean six different scenarios with different regulatory implications, and acting on ambiguous intent creates liability. Her work has been about building the logic layer that sits between user utterances and system actions: disambiguation funnels that verify state before escalation, explicit confirmation loops that prevent stale consent in async channels, and escalation policies that know when automation must stop. She doesn't design conversations — she defines the decision boundaries that make automation safe at scale.

---

## Language That Must Appear

- "Decision boundaries and intent eligibility"
- "Confidence thresholds and fallback logic"
- "Escalation policies" and "escalation discipline"
- "Containment rate" and "false positive reduction"
- "Regulatory alignment" and "compliance architecture"
- "Verification-first approach" and "explicit confirmation as safety lock"
- "Disambiguation funnel" and "progressive verification"
- "Stale consent risk" and "session management"
- "I defined," "I built," "I refused to allow," "I specified"
- "Truth before tickets" and "verified state"
- "Correctness over speed"
- "Behavior ownership" and "systems thinking"

---

## Language That Must Not Appear

**UX execution framing:**
- "I designed flows" → Replace with "I defined decision logic"
- "Crafting microcopy" → Replace with "Specifying system responses"
- "User journey mapping" → Replace with "State verification sequence"
- "Architecting conversations" → Replace with "Building disambiguation funnels"

**Designer job title signals:**
- "Lead Designer," "Conversational Designer," "UX Designer"
- Replace with "AI Systems & Automation Behavior Owner" or "Automation Behavior Architect"

**Vague impact claims:**
- "Improved user experience" without metrics
- "Reduced confusion" without containment rates
- "Better clarity" without false positive data

**UX portfolio vibes:**
- "Delightful," "seamless," "frictionless" (unless tied to operational outcome)
- "User empathy" as standalone value (vs. risk mitigation)
- "Storytelling" or "narrative design" (unless about decision logic narrative)

**Hedge words:**
- "Somewhat," "quite," "relatively," "potentially," "fairly"
- Speak definitively: "This reduced escalations" (if true) or "This was designed to reduce escalations" (if metric unknown)

---

## Homepage Structure Recommendation

1. **Hero Section** — Immediate positioning
   - Headline: What she builds (automation systems that know when to stop)
   - Sub-headline: The specific capability (decision boundaries for financial AI)
   - Proof anchor: Scale or impact metric (70+ live transaction flows, Tier I bank production system)

2. **Manifesto / Philosophy Block**
   - Single paragraph: The through-line insight (gap between user utterances and system permissions)
   - Pull quote: "I build automation that knows when to stop."
   - Three pillars displayed as capabilities, not values: Behavior Ownership, Risk-Aware Automation, Measurable Outcomes

3. **Case Study Grid** — Maximum 3 projects
   - Lead: CS2 (Designing for Panic) — narrative entry point, strategic framing
   - Support: CS1 (Audit-Safe Conversations) — technical depth, systems proof
   - Scale anchor: ICICI Bank Voicebot — production system, 70+ flows
   - **Exclude**: Dottie (speculative), Vihar (no automation), Bhopal Bikes (no automation), SmartCoin (unless reframed as automation behavior design)

4. **Capabilities Section** (replaces "About")
   - What she owns (not what she designs): Decision boundaries, Escalation taxonomies, Verification sequences, Compliance alignment
   - Who she collaborates with: Engineering, Data Science, Compliance, Product
   - Tools/frameworks she works in: NLU intent hierarchies, Slot-filling logic, Fallback strategies, Session management

5. **Contact / Next Steps**
   - Simple, minimal
   - No personality statements ("I love solving complex problems")
   - Direct: Email, LinkedIn, optionally calendar link

---

## Case Study Priority Order

### 1. Lead with CS2: "Designing for Panic" (Failed UPI Debit)
**Why this leads:**
- Opens with strategic framing: "We stopped building a support bot. We built a diagnostic engine."
- Demonstrates behavior ownership through the 5-gate logic funnel (regulatory window checking, debit verification, payment classification)
- Shows risk-aware automation: "Escalation is no longer triggered by emotion. It is triggered by verified system state."
- Has philosophical depth: the shift from "inquiry" (what happened?) to "validation" (here's what we see)
- Positions her as someone who thinks in systems, not flows

### 2. Follow with CS1: "Audit-Safe Conversations" (Technical Proof Layer)
**Why this is second:**
- Provides the detailed technical evidence for what CS2 introduces strategically
- Shows disambiguation architecture: 5-step funnel, multi-dispute resolution taxonomy
- Demonstrates compliance thinking: UI elements as visible compliance, stale consent prevention
- Includes the decision matrix and persona-based control thresholds
- Proves she can go deep on system logic, not just strategic framing

### 3. Close with ICICI Bank Voicebot (Scale & Production Anchor)
**Why this is third:**
- Validates everything above with production scale: 70+ live use cases
- Provides the credibility anchor: Tier I Indian bank, transactional banking on call
- Shows cross-functional collaboration: ML engineers, compliance, solution architects
- Demonstrates operational impact: "dramatic reduction in call center dependency"
- Brief format works here — the detailed proof is already established in CS1/CS2

**Projects to exclude from main grid:**
- **Dottie**: Speculative project, not production work, dilutes credibility
- **SmartCoin**: Currently framed as UX research, would need complete rewrite to focus on Chillar feature as automation behavior design
- **Vihar**: Service design research, no automation component, wrong positioning
- **Bhopal Bikes**: Communication design research, no automation component, wrong positioning

---

## Open Questions for Agent 2 (Researcher)

These evidence gaps would strengthen positioning if data exists:

1. **ICICI Bank Voicebot metrics:**
   - Call deflection rate or containment rate (% of calls resolved without human transfer)
   - Which of the 70+ use cases have highest success rates?
   - Reduction in average handle time or call center volume

2. **WhatsApp Dispute Resolution (CS1/CS2) scale:**
   - How many users does this system serve?
   - Monthly transaction volume handled?
   - SR creation reduction (before/after disambiguation funnel implementation)?
   - Incorrect SR rate or false positive rate?

3. **Regulatory compliance outcomes:**
   - Zero compliance violations claim?
   - Audit trail completeness rate?
   - Stale consent incidents prevented?

4. **Chillar (SmartCoin) automation behavior:**
   - Was this feature implemented?
   - Savings adoption rate among borrowers?
   - Automated savings volume?

5. **Team structure context:**
   - What size engineering / data science teams did she collaborate with?
   - Reporting structure (to Product? To AI/ML org? To Operations?)
   - Cross-functional working model evidence

6. **Persona-based threshold differentiation (CS1):**
   - Were retail vs. SME thresholds actually implemented distinctly?
   - Different containment rates by persona?
   - Session timeout impact on stale consent reduction?

7. **Disambiguation funnel effectiveness (CS1):**
   - Reduction in misclassified intents after 5-step funnel implementation?
   - User completion rate through funnel vs. dropout rate?
   - Agent escalation rate before/after?

**Instruction for Agent 2:** If these metrics don't exist in source material, flag them as "recommended additions" but do NOT invent numbers. The final portfolio can be honest about "designed to reduce X" vs. "reduced X by Y%" where data is unavailable.
